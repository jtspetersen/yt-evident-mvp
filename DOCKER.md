# Docker Architecture - Evident Video Fact Checker

Technical documentation for the Evident Video Fact Checker Docker Compose stack.

## Table of Contents

- [Architecture Overview](#architecture-overview)
- [Service Descriptions](#service-descriptions)
- [Volume Strategy](#volume-strategy)
- [Environment Variables](#environment-variables)
- [Healthchecks](#healthchecks)
- [GPU vs CPU Modes](#gpu-vs-cpu-modes)
- [Networking](#networking)
- [Performance Considerations](#performance-considerations)
- [Security](#security)
- [Customization](#customization)

---

## Architecture Overview

### Service Dependency Graph

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│                     Docker Compose Stack                    │
│                                                             │
│  ┌──────────┐     ┌──────────┐     ┌──────────────┐        │
│  │  Redis   │────▶│ SearXNG  │────▶│     App      │        │
│  └──────────┘     └──────────┘     └──────────────┘        │
│       │                                     │               │
│       │                                     │               │
│       │                                     ▼               │
│       │           ┌──────────────────────────────┐         │
│       └──────────▶│        Ollama (LLM)          │         │
│                   └──────────────────────────────┘         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
         │                                         │
         ▼                                         ▼
   Host Port 8080                            Host Port 11434
   (SearXNG API)                             (Ollama API)
```

### Startup Order

1. **Redis** starts first (SearXNG cache)
   - Healthcheck: `redis-cli ping`
   - Waits: 10 seconds
2. **Ollama** starts (LLM service)
   - Healthcheck: `curl /api/tags`
   - Waits: 60 seconds
3. **SearXNG** starts after Redis is healthy
   - Healthcheck: `curl /search?q=test`
   - Waits: 30 seconds
4. **App** starts after Ollama + SearXNG are healthy
   - No healthcheck (one-off runs)

### File Structure

```
evident-video-fact-checker/
├── docker-compose.yml          # Base service definitions (CPU-only)
├── docker-compose.gpu.yml      # GPU overrides (nvidia runtime)
├── Dockerfile                  # App container image
├── .dockerignore               # Build exclusions
├── .env                        # Environment variables (gitignored)
├── .env.example                # Environment template
│
├── data/                       # Runtime data (volumes)
│   ├── cache/                  # URL cache (7-day TTL)
│   ├── runs/                   # Output artifacts
│   ├── store/                  # JSONL append-only logs
│   ├── inbox/                  # Input transcripts
│   ├── logs/                   # Run logs
│   └── ollama/                 # Ollama model storage
│
├── searxng/                    # SearXNG config (generated by setup.py)
│   ├── settings.yml            # Main config
│   └── limiter.toml            # Rate limiting
│
├── Makefile                    # Operations interface
└── setup.py                    # Interactive setup wizard
```

---

## Service Descriptions

### 1. Redis (Cache)

**Image:** `redis:7-alpine`

**Purpose:** In-memory cache for SearXNG search results

**Configuration:**
- Port: 6379 (internal only, not exposed to host)
- Volume: `redis-data` (Docker-managed volume)
- Healthcheck: Every 10s, `redis-cli ping`

**Why Redis?**
- SearXNG requires Redis for caching search results
- Improves search performance (reduces duplicate API calls)
- Small memory footprint (typically <100 MB)

### 2. Ollama (LLM Service)

**Image:** `ollama/ollama:latest`

**Purpose:** Local LLM inference for extract/verify/write stages

**Configuration:**
- Port: 11434 (exposed to host for debugging)
- Volume: `./data/ollama:/root/.ollama` (model storage)
- Environment:
  - `OLLAMA_HOST=0.0.0.0` (listen on all interfaces)
  - `NVIDIA_VISIBLE_DEVICES=all` (GPU mode only)
- Healthcheck: Every 30s, `curl /api/tags`

**Model Storage:**
- Models stored in `data/ollama/` on host
- Persists between container restarts
- Large models (30B) can be 15-20 GB each

**GPU Mode:**
- Requires `docker-compose.gpu.yml` overlay
- Uses NVIDIA Container Runtime
- Enables CUDA acceleration

### 3. SearXNG (Metasearch)

**Image:** `searxng/searxng:latest`

**Purpose:** Privacy-focused metasearch for evidence retrieval

**Configuration:**
- Port: 8080 (exposed to host)
- Volume: `./searxng:/etc/searxng:rw` (config)
- Environment:
  - `SEARXNG_BASE_URL=http://localhost:8080`
  - `BIND_ADDRESS=0.0.0.0:8080`
- Depends on: Redis (must be healthy)
- Healthcheck: Every 20s, `curl /search?q=test&format=json`

**Search Engines:**
- Google, Bing, DuckDuckGo, Wikipedia (configurable in `searxng/settings.yml`)

**Rate Limiting:**
- Configured in `searxng/limiter.toml`
- Protects against bot detection
- Uses Redis for distributed rate limiting

### 4. App (Python Pipeline)

**Image:** Built from `Dockerfile` (Python 3.11-slim)

**Purpose:** 7-stage fact-checking pipeline

**Configuration:**
- Port: None (runs as one-off task)
- Volumes:
  - `./data/cache:/app/cache` (URL cache)
  - `./data/runs:/app/runs` (outputs)
  - `./data/store:/app/store` (JSONL logs)
  - `./data/inbox:/app/inbox` (inputs)
  - `./data/logs:/app/logs` (run logs)
  - `./config.yaml:/app/config.yaml:ro` (read-only)
- Environment: Loaded from `.env` file
- Depends on: Ollama + SearXNG (must be healthy)

**User:**
- Runs as non-root user `app` (UID 1000) for security

**Command:**
- Default: `python -m app.main --help`
- Override via `make run ARGS="..."`

---

## Volume Strategy

### Docker-Managed Volumes

**redis-data:**
- Purpose: Redis persistence (search cache)
- Managed by: Docker
- Location: Docker's volume directory (varies by OS)
- Cleanup: `docker compose down -v` removes it

### Host Bind Mounts

**data/cache:**
- Purpose: URL cache (HTTP responses, 7-day TTL)
- Format: SHA256 hashed filenames
- Size: Grows over time (typically <1 GB)
- Cleanup: `make clean-cache`

**data/runs:**
- Purpose: Pipeline outputs (artifacts, logs, manifests)
- Format: Timestamped directories (YYYYMMDD_HHMMSS__channel__video)
- Size: Grows with each run (~5-50 MB per run)
- Cleanup: Manual (or `make clean-all`)

**data/store:**
- Purpose: Append-only JSONL logs (run_index, creator_profiles)
- Format: JSONL (one JSON object per line)
- Size: Small (<10 MB typically)
- Cleanup: Manual (or `make clean-all`)

**data/inbox:**
- Purpose: Input transcripts (.txt or .md files)
- Format: Plain text or Markdown
- Size: User-controlled
- Cleanup: Manual

**data/logs:**
- Purpose: Run logs (run.log files)
- Format: Plain text
- Size: Small (<1 MB per run)
- Cleanup: Manual (or `make clean-all`)

**data/ollama:**
- Purpose: Ollama model storage
- Format: Binary model files
- Size: Large (5-20 GB per model, 50+ GB total)
- Cleanup: `make models-rm MODEL=<name>` or `make clean-all`

**searxng/**:
- Purpose: SearXNG configuration
- Files:
  - `settings.yml` (main config, generated by setup.py)
  - `limiter.toml` (rate limiting, generated by setup.py)
- Size: <1 KB
- Cleanup: Regenerate via `python setup.py`

---

## Environment Variables

All environment variables use the `EVIDENT_*` prefix to avoid conflicts.

### Configuration Hierarchy

1. **Shell environment variables** (highest precedence)
2. **`.env` file** (loaded by python-dotenv)
3. **`config.yaml`** (lowest precedence, backward compatible)

### Available Variables

| Variable | Type | Default | Description |
|----------|------|---------|-------------|
| `EVIDENT_OLLAMA_BASE_URL` | URL | `http://ollama:11434` | Ollama API endpoint (Docker internal) |
| `EVIDENT_SEARXNG_BASE_URL` | URL | `http://searxng:8080` | SearXNG API endpoint (Docker internal) |
| `EVIDENT_MODEL_EXTRACT` | string | `qwen3:8b` | Model for claim extraction |
| `EVIDENT_MODEL_VERIFY` | string | `qwen3:30b` | Model for claim verification |
| `EVIDENT_MODEL_WRITE` | string | `gemma3:27b` | Model for outline/script generation |
| `EVIDENT_TEMPERATURE_EXTRACT` | float | `0.1` | Temperature for extraction (0.0-1.0) |
| `EVIDENT_TEMPERATURE_VERIFY` | float | `0.0` | Temperature for verification (0.0-1.0) |
| `EVIDENT_TEMPERATURE_WRITE` | float | `0.5` | Temperature for writing (0.0-1.0) |
| `EVIDENT_GPU_ENABLED` | bool | `false` | GPU acceleration enabled |
| `EVIDENT_GPU_MEMORY_GB` | float | `0.0` | GPU VRAM in GB (informational) |
| `EVIDENT_RAM_GB` | float | `0.0` | System RAM in GB (informational) |
| `EVIDENT_MAX_CLAIMS` | int | `25` | Maximum claims to extract per video |
| `EVIDENT_CACHE_TTL_DAYS` | int | `7` | URL cache TTL in days |
| `EVIDENT_LOG_LEVEL` | string | `INFO` | Logging level (DEBUG/INFO/WARNING/ERROR) |

### Example .env File

```bash
# Service URLs (Docker internal network)
EVIDENT_OLLAMA_BASE_URL=http://ollama:11434
EVIDENT_SEARXNG_BASE_URL=http://searxng:8080

# Model selections
EVIDENT_MODEL_EXTRACT=qwen3:8b
EVIDENT_MODEL_VERIFY=qwen3:30b
EVIDENT_MODEL_WRITE=gemma3:27b

# Model temperatures
EVIDENT_TEMPERATURE_EXTRACT=0.1
EVIDENT_TEMPERATURE_VERIFY=0.0
EVIDENT_TEMPERATURE_WRITE=0.5

# Hardware configuration (set by setup.py)
EVIDENT_GPU_ENABLED=true
EVIDENT_GPU_MEMORY_GB=24.0
EVIDENT_RAM_GB=64.0

# Pipeline budgets
EVIDENT_MAX_CLAIMS=25
EVIDENT_CACHE_TTL_DAYS=7

# Logging
EVIDENT_LOG_LEVEL=INFO
```

---

## Healthchecks

Healthchecks ensure services are ready before dependent services start.

### Redis

```yaml
healthcheck:
  test: ["CMD", "redis-cli", "ping"]
  interval: 10s
  timeout: 5s
  retries: 3
  start_period: 10s
```

**Logic:**
- Every 10 seconds, runs `redis-cli ping`
- Expects response: `PONG`
- Allows 10 seconds for initial startup
- Fails after 3 consecutive failures

### Ollama

```yaml
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
  interval: 30s
  timeout: 10s
  retries: 5
  start_period: 60s
```

**Logic:**
- Every 30 seconds, fetches `/api/tags` endpoint
- Expects HTTP 200 response
- Allows 60 seconds for model loading
- Fails after 5 consecutive failures (2.5 minutes)

**Why 60s start_period?**
- Ollama needs time to initialize CUDA (GPU mode)
- Large models may need preloading

### SearXNG

```yaml
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost:8080/search?q=test&format=json"]
  interval: 20s
  timeout: 5s
  retries: 3
  start_period: 30s
```

**Logic:**
- Every 20 seconds, performs a test search
- Expects HTTP 200 + valid JSON
- Allows 30 seconds for Redis connection
- Fails after 3 consecutive failures (1 minute)

### App

**No healthcheck** - runs as one-off task, not a long-lived service.

**Dependency Logic:**
```yaml
depends_on:
  ollama:
    condition: service_healthy
  searxng:
    condition: service_healthy
```

App waits for Ollama + SearXNG to report `healthy` before starting.

---

## GPU vs CPU Modes

### CPU-Only Mode

**Activate:**
```bash
docker compose -f docker-compose.yml up
# Or simply:
make start
```

**When to use:**
- No GPU available
- Testing/development
- Limited VRAM

**Performance:**
- Extract (qwen3:8b): ~30-60s per chunk
- Verify (qwen3:30b): ~2-5 minutes per claim (CPU inference is slow)
- Write (gemma3:27b): ~3-5 minutes

**RAM Requirements:**
- qwen3:30b needs ~32 GB RAM for CPU inference
- Slower than GPU by 10-50x

### NVIDIA GPU Mode

**Activate:**
```bash
docker compose -f docker-compose.yml -f docker-compose.gpu.yml up
# Or simply:
make start  # Auto-detects NVIDIA GPU
```

**Requirements:**
- NVIDIA GPU with CUDA support (RTX 3000/4000 series, A-series, etc.)
- nvidia-docker (NVIDIA Container Toolkit)
- Sufficient VRAM for models

**Performance:**
- Extract (qwen3:8b): ~5-15s per chunk
- Verify (qwen3:30b): ~15-45s per claim
- Write (gemma3:27b): ~30-60s

**VRAM Requirements:**
- qwen3:8b: ~6 GB
- qwen3:30b: ~20 GB
- gemma3:27b: ~18 GB

**GPU Allocation:**
```yaml
# docker-compose.gpu.yml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all  # Use all available GPUs
          capabilities: [gpu]
```

**Environment:**
```yaml
environment:
  - NVIDIA_VISIBLE_DEVICES=all
  - NVIDIA_DRIVER_CAPABILITIES=compute,utility
```

### AMD GPU Mode (ROCm)

**Activate:**
```bash
docker compose -f docker-compose.yml -f docker-compose.amd.yml up
# Or simply:
make start  # Auto-detects AMD GPU
```

**Requirements:**
- AMD GPU with ROCm support (RX 6000/7000 series: 6700 XT, 7900 XTX, etc.)
- ROCm 6.1+ installed on host
- Windows: WSL2 with ROCm OR native Windows ROCm support
- Linux: Native ROCm installation
- Ollama with ROCm support (may need custom image)

**Supported AMD GPUs:**
- RX 7900 XTX (24 GB VRAM) ✓
- RX 7900 XT (20 GB VRAM) ✓
- RX 7800 XT (16 GB VRAM) ✓
- RX 7700 XT (12 GB VRAM) ✓
- RX 6900 XT, 6800 XT, etc.

**Performance:**
- Similar to NVIDIA GPUs with equivalent VRAM
- RX 7900 XTX ~= RTX 4080 for LLM inference

**VRAM Requirements:**
- Same as NVIDIA (qwen3:8b ~6 GB, qwen3:30b ~20 GB, etc.)

**GPU Allocation:**
```yaml
# docker-compose.amd.yml
devices:
  - /dev/kfd:/dev/kfd    # AMD GPU compute device
  - /dev/dri:/dev/dri    # Direct rendering infrastructure

environment:
  - HSA_OVERRIDE_GFX_VERSION=11.0.0  # For RX 7900 XTX (gfx1100)
  - ROCM_PATH=/opt/rocm
  - HIP_VISIBLE_DEVICES=0
```

**Windows Setup:**
ROCm on Windows requires WSL2:
1. Install WSL2: `wsl --install`
2. Install Ubuntu in WSL2
3. Install ROCm in WSL2: https://rocm.docs.amd.com/
4. Configure Docker Desktop to use WSL2 backend
5. Run setup wizard: `python setup.py`

**Linux Setup:**
1. Install ROCm 6.1+: https://rocm.docs.amd.com/
2. Verify installation: `rocm-smi`
3. Add user to `video` and `render` groups
4. Run setup wizard: `python setup.py`

**Important Notes:**
- Standard `ollama/ollama:latest` may not include ROCm support
- May need to use `ollama/ollama:rocm` (if available) or build custom image
- ROCm support on Windows is newer and may have limitations
- Performance depends on ROCm version and GPU driver compatibility

### Switching Modes

**Enable NVIDIA GPU:**
```bash
# Ensure nvidia-docker installed
docker info | grep -i runtime  # Should show "nvidia"

# Restart with GPU
make stop
make start  # Auto-detects NVIDIA GPU
```

**Enable AMD GPU:**
```bash
# Ensure ROCm installed
rocm-smi  # Should show GPU info

# Restart with GPU
make stop
make start  # Auto-detects AMD GPU
```

**Disable GPU (force CPU mode):**
```bash
# Use base compose file only
docker compose -f docker-compose.yml up
```

---

## Networking

### Docker Network

**Network:** `evident-net` (bridge driver)

**Purpose:**
- Isolates services from host network
- Enables service-to-service communication via service names

**Internal DNS:**
- `redis` → `evident-redis` container
- `ollama` → `evident-ollama` container
- `searxng` → `evident-searxng` container
- `app` → `evident-app` container (one-off)

### Port Mapping

| Service | Internal Port | Host Port | Purpose |
|---------|---------------|-----------|---------|
| Redis | 6379 | - | Cache (internal only) |
| Ollama | 11434 | 11434 | LLM API (debugging) |
| SearXNG | 8080 | 8080 | Search API (required) |
| App | - | - | One-off runs |

### Service URLs

**From App Container:**
- Ollama: `http://ollama:11434`
- SearXNG: `http://searxng:8080`
- Redis: `redis://redis:6379/0`

**From Host Machine:**
- Ollama: `http://localhost:11434`
- SearXNG: `http://localhost:8080`
- Redis: Not accessible (no port mapping)

---

## Performance Considerations

### Model Selection

**Extract Stage:**
- Recommended: `qwen3:8b` (fast, good quality)
- Alternative: `phi3:mini` (faster, lower quality)

**Verify Stage:**
- Recommended: `qwen3:30b` (best reasoning)
- Alternative: `qwen3:14b` (faster, good quality)
- Minimum: `qwen3:8b` (fastest, acceptable quality)

**Write Stage:**
- Recommended: `gemma3:27b` (best writing)
- Alternative: `gemma3:12b` (faster)
- Minimum: `llama3:8b` (fastest)

### Disk I/O

**Bottlenecks:**
- Model loading from `data/ollama/`
- URL cache reads from `data/cache/`
- Output writes to `data/runs/`

**Optimizations:**
- Use SSD for `data/ollama/` (models are 5-20 GB)
- Use SSD for `data/cache/` (reduces fetch latency)
- HDD acceptable for `data/runs/` (sequential writes)

### Memory

**Docker Memory Limits:**
```yaml
# Add to docker-compose.yml if needed
services:
  ollama:
    mem_limit: 32g  # Limit Ollama to 32 GB
```

**Swap:**
- CPU inference may use swap for large models
- Ensure swap enabled on Linux (8+ GB recommended)

### Parallelism

**Current Implementation:**
- Sequential claim verification (one at a time)
- No parallel Ollama calls

**Future Optimization:**
- Parallel verification (requires Ollama queue management)
- Batch inference (future Ollama feature)

---

## Security

### Container Security

**Non-Root User:**
```dockerfile
# Dockerfile
RUN useradd -m -u 1000 -s /bin/bash app
USER app
```

**Why?**
- Limits privilege escalation
- Follows principle of least privilege

### Network Security

**No Unnecessary Ports:**
- Redis: Not exposed to host
- Only Ollama and SearXNG need host access

**Bridge Network:**
- Services isolated from host network by default

### Secrets Management

**.env File:**
- Gitignored (not committed to repo)
- Contains no real secrets (only config)

**SearXNG Secret Key:**
- Auto-generated random token (64 hex chars)
- Stored in `searxng/settings.yml` (gitignored)

### Data Privacy

**Local-Only:**
- All data stays on your machine
- No external API calls except web searches (via SearXNG)
- Ollama runs locally (no cloud)

**SearXNG Privacy:**
- Proxies search requests (hides your IP from search engines)
- No search logs stored
- No tracking cookies

---

## Customization

### Adding Models

```bash
make models-pull MODEL=llama3:70b
```

Update `.env`:
```bash
EVIDENT_MODEL_VERIFY=llama3:70b
```

### Custom Ollama Image

Build your own Ollama image with pre-downloaded models:

```dockerfile
FROM ollama/ollama:latest
RUN ollama pull qwen3:8b
RUN ollama pull qwen3:30b
RUN ollama pull gemma3:27b
```

Update `docker-compose.yml`:
```yaml
services:
  ollama:
    build: ./ollama-custom
```

### Custom SearXNG Engines

Edit `searxng/settings.yml`:

```yaml
engines:
  - name: google
    disabled: false
  - name: scholar
    disabled: false  # Enable Google Scholar
  - name: arxiv
    disabled: false  # Enable arXiv
```

Restart SearXNG:
```bash
docker compose restart searxng
```

### Resource Limits

**Limit Ollama CPU:**
```yaml
services:
  ollama:
    cpus: '8.0'  # Max 8 CPUs
```

**Limit Ollama Memory:**
```yaml
services:
  ollama:
    mem_limit: 32g  # Max 32 GB RAM
```

### Custom Network

Replace `evident-net` with external network:

```yaml
networks:
  evident-net:
    external: true
    name: my-custom-network
```

---

## Troubleshooting

### Check Service Logs

```bash
make logs              # All services
make logs-ollama       # Ollama only
make logs-searxng      # SearXNG only
```

### Check Service Health

```bash
make status
docker compose ps
```

### Restart Services

```bash
make restart           # All services
docker compose restart ollama  # Single service
```

### Rebuild App Image

```bash
make build             # Use cache
make build-no-cache    # Fresh build
```

### Reset Everything

```bash
make clean-all  # WARNING: Deletes models and data
```

---

## Summary

**Docker Stack:**
- 4 services: Redis, Ollama, SearXNG, App
- Automatic healthchecks and startup ordering
- GPU support via nvidia-docker

**Volumes:**
- `data/` for all runtime data (cache, runs, store, inbox, logs, ollama)
- `searxng/` for SearXNG config

**Configuration:**
- `.env` file for environment variables
- `config.yaml` for backward compatibility

**Commands:**
- `make setup` - First-time setup
- `make start/stop` - Service management
- `make run/review` - Pipeline execution
- `make models` - Model management

**For More Info:**
- [MIGRATION.md](MIGRATION.md) - Migration guide
- `make help` - Command reference
- GitHub Issues - Support

---

**Questions? Issues? Contributions?**

Open an issue on GitHub or submit a pull request.
